% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/httr-helpers.R
\name{safeGET}
\alias{safeGET}
\title{Safer version of \code{\link[httr:GET]{httr::GET()}}}
\usage{
safeGET(url = NULL, config = list(), timeout = httr::timeout(5), ...,
  handle = NULL)
}
\arguments{
\item{url}{the url of the page to retrieve}

\item{config}{Additional configuration settings such as http
authentication (\code{\link{authenticate}}), additional headers
(\code{\link{add_headers}}), cookies (\code{\link{set_cookies}}) etc.
See \code{\link{config}} for full details and list of helpers.}

\item{timeout}{a call to \code{\link[httr:timeout]{httr::timeout()}}. Default timeout is \code{5} seconds.}

\item{...}{Further named parameters, such as \code{query}, \code{path}, etc,
passed on to \code{\link{modify_url}}. Unnamed parameters will be combined
with \code{\link{config}}.}

\item{handle}{The handle to use with this request. If not
supplied, will be retrieved and reused from the \code{\link{handle_pool}}
based on the scheme, hostname and port of the url. By default \pkg{httr}
requests to the same scheme/host/port combo. This substantially reduces
connection time, and ensures that cookies are maintained over multiple
requests to the same host. See \code{\link{handle_pool}} for more
details.}
}
\description{
Scraping the web is fraught with peril. URLs die; networks get disrupted
and best laid plans for building a corups from links can quickly go awry.
Use this funtion to mitigate some of the pain of retrieving web resoures.
}
\details{
This is a thin wrapper for \code{\link[httr:GET]{httr::GET()}} using \code{\link[purrr:safely]{purrr::safely()}} that will
either return a \code{httr} \code{response} object or \code{NULL} if there was an error.
If you need the reason for the error (e.g. \code{Could not resolve host...})
you should write your own wrapper.
}
